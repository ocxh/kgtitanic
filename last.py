# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xh53hbu7IUzHyg9naD221zCT32nAaMcm
"""

import pandas as pd

#데이터 읽어오기
train = pd.read_csv("./train.csv")
test = pd.read_csv("./test.csv")

#데이터 프레임 확인
#train.head()
#test.head()
#train.describe()

#누락된 값 확인(훈련 데이터)
print("train.isnull().sum")
train.isnull().sum()

#누락된 값 확인(테스트 데이터)
print("test.isnull().sum()")
test.isnull().sum()

#전처리(Sex,Age, Embarked, Name, Cabin, Fare, SibSp, Parch, Ticket, Pclass)(9)  not(PassengerId, Survived)(2)

#Sex=======================
#성별을 숫자로 매핑
sex_map = {"male": 0, "female": 1}
train['Sex'] = train['Sex'].map(sex_map)
test['Sex'] = test['Sex'].map(sex_map)

#Age(추가 시 점수 떨어짐)==============
train_test_data = [train, test] 
#누락된 값 평균으로 채우기
train["Age"] = train["Age"].fillna(train["Age"].mean())
test["Age"] = test["Age"].fillna(test["Age"].mean())
#나이별로 범위를 만들었음(범위를 아래와 같이 했을 때 submission점수 1 가까이 상승)
"""
for dataset in train_test_data:
  dataset.loc[dataset['Age'] <= 16, 'Age'] = 0
  dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1
  dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2
  dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3
  dataset.loc[ dataset['Age'] > 62, 'Age'] = 4
"""

for dataset in train_test_data:
  dataset.loc[dataset['Age'] <= 10, 'Age'] = 0
  dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'Age'] = 1
  dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'Age'] = 2
  dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age'] = 3
  dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age'] = 4
  dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age'] = 5
  dataset.loc[ dataset['Age'] > 60, 'Age'] = 6


#Pclass
#one hot encoding 적용 하나 안하나 점수차이 X
#one-hot encoding 적용
'''
train['Pclass_3']=(train['Pclass']==3)
train['Pclass_2']=(train['Pclass']==2)
train['Pclass_1']=(train['Pclass']==1)

test['Pclass_3']=(test['Pclass']==3)
test['Pclass_2']=(test['Pclass']==2)
test['Pclass_1']=(test['Pclass']==1)

train=train.drop(columns='Pclass')
test=test.drop(columns='Pclass')
'''



#Family (새로운 칼럼, 있으나 없으나 점수차이 X) => (Solo칼럼을 만들기 위해서)===========
train['Family'] = 1 + train['SibSp'] + train['Parch']
test['Family'] = 1 + test['SibSp'] + test['Parch']

#Solo (새로운 칼럼, 추가 시 점수 오름)============
train['Solo'] = (train['Family'] == 1)
test['Solo'] = (test['Family'] == 1)

#Parch (제거 시 점수 떨어짐, 그대로 유지)=============

#SibSp(있으나 없으나 관계 X)==========

#Name==========
train_test_data = [train, test] 
for dataset in train_test_data:
    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

#Title==========
#Name을 통한새로운 칼럼 생성
title_mapping = {"Mr": 0, "Miss": 1, "Mrs": 2, 
                 "Master": 3, "Dr": 3, "Rev": 3, "Col": 3, "Major": 3, "Mlle": 3,"Countess": 3,
                 "Ms": 3, "Lady": 3, "Jonkheer": 3, "Don": 3, "Dona" : 3, "Mme": 3,"Capt": 3,"Sir": 3 }
for dataset in train_test_data:
    dataset['Title'] = dataset['Title'].map(title_mapping)

#Name 칼럼 제거
train.drop('Name', axis=1, inplace=True)
test.drop('Name', axis=1, inplace=True)

#Embarked ==============
#Embarked가 비어있으면 S를 채운다.
for dataset in train_test_data:
    dataset['Embarked'] = dataset['Embarked'].fillna('S')
# S는 0, C는 1, Q는 2라고 mapping
embarked_mapping = {"S": 0, "C": 1, "Q": 2}
for dataset in train_test_data:
    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)

#Fare ===============
#누락된 값 평균으로 채우기
train["Fare"] = train["Fare"].fillna(train["Fare"].mean())
test["Fare"] = test["Fare"].fillna(test["Fare"].mean())
#Fare을 범위별 매핑하기
#Fare의 범위는 더 자잘하게 나누어도 submission점수엔 변화 없었음
for dataset in train_test_data:
    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2
    dataset.loc[ dataset['Fare'] > 100, 'Fare'] = 3




#Cabin===============
#소숫점으로 mapping한 이유는 숫자의 범위가 너무 크면 안되기 때문이다.
cabin_mapping = {"A": 0, "B": 0.4, "C": 0.8, "D": 1.2, "E": 1.6, "F": 2, "G": 2.4, "T": 2.8}
for dataset in train_test_data:
    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)
# Cabin의 Missing value들은 각 클래스별 Cabin의 중간값을 넣어준다.
#누락된 값 채우기 
train["Cabin"] = train["Cabin"].fillna(train["Cabin"].mean())
test["Cabin"] = test["Cabin"].fillna(test["Cabin"].mean())

#값 제거=========================
#생존과 관계없는 값들 제거
train = train.drop(['PassengerId','Ticket'],axis=1)
test = test.drop(['Ticket'],axis=1)
#추가 시 점수가 떨어지기에 제거
#train = train.drop([''], axis=1)
#test = test.drop([''], axis=1)

train.isnull().sum()

test.isnull().sum()

#train_input, train_target
train_input = train.drop(['Survived'], axis=1)
train_target = train['Survived']

#그레이디언트 부스팅 Scores:0.78708 (그리드서치 적용해도 점수차이 X)
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_validate
import numpy as np
#n_estimators와 learning_rate의 최적의 값을 찾은 결과 100과 0.01일 때 가장 높은 정확도를 보임
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, random_state=42)
gb.fit(train_input, train_target)

scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

predict = gb.predict(test.drop(["PassengerId"], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

#로지스틱 하이퍼파라미터 찾기
#차이가 없었음
from sklearn.linear_model import LogisticRegression
Crange = [1, 10, 20, 30, 40, 50]
scores = []
for i in Crange:
  lr = LogisticRegression(C=i, max_iter=1000)

  lr.fit(train_input, train_target)
  scores.append(lr.score(train_input, train_target))


lr.coef_

predict = lr.predict(test.drop(['PassengerId'], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

import numpy as np
import matplotlib.pyplot as plt

plt.plot(Crange, scores)
plt.xlabel('C')
plt.ylabel('score')
plt.show()

#로지스틱 Score: 0.78468
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=1, max_iter=1000)

lr.fit(train_input, train_target)
print(lr.score(train_input, train_target))

lr.coef_

predict = lr.predict(test.drop(['PassengerId'], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

#결정트리 Score: 0.76315 (그리드서치 적용)
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

params = {'min_impurity_decrease':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

dt = DecisionTreeClassifier(max_depth=4, random_state=42)
gs = GridSearchCV(dt, params, n_jobs=-1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_

predict = dt.predict(test.drop(['PassengerId'], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

#랜덤포레스트 Score: 0.75598 (그리드서치 적용)
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import numpy as np

params = {'min_impurity_decrease':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

dt = RandomForestClassifier(n_jobs=-1, random_state=42)
gs = GridSearchCV(dt, params, n_jobs=-1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_

predict = dt.predict(test.drop(['PassengerId'], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

#엑스트라 트리 Score: 0.75358 => Score: 0.77990 (그리드 서치 적용)
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
import numpy as np

params = {'min_impurity_decrease':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

dt = ExtraTreesClassifier(n_jobs=-1, random_state=42)
gs = GridSearchCV(dt, params, n_jobs=-1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_

predict = dt.predict(test.drop(['PassengerId'], axis=1))
rs =pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predict
})

#결과 submission파일 생성
print(rs)
rs.to_csv('result.csv', index=False)